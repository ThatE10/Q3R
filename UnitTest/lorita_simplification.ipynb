{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce05a99-ecb2-4356-80a7-01b092b8d50d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enguye17\\PycharmProjects\\QuaRS\\new_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from timm import create_model\n",
    "from timm.models import ResNet\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c803a50-f6c0-4eb1-b7a4-bacdaeed99c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "import main_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d9be99-771a-4ed5-8f1e-7d785b1c0f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'main_helper' from 'C:\\\\Users\\\\enguye17\\\\PycharmProjects\\\\QuaRS\\\\main_helper.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(main_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04599cc0-f1d7-4edf-8140-47bfe0395ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "config = argparse.Namespace(\n",
    "    dataset='CIFAR10',\n",
    "    model='VIT_Tiny',\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    epoch=50,\n",
    "    technique='LoRITa',\n",
    "    rectangular_mode=True,\n",
    "    target_rank=4,\n",
    "    lmbda=0.01,\n",
    "    depth_lorita=4,\n",
    "    weight_decay_alpha=0.00,\n",
    "    dropout=0.0,\n",
    "    threads=4,\n",
    "    augmentation=False,\n",
    "    target_modules=['Q', 'K', 'V','FC1','FC2'],\n",
    "    DEVICE = torch.device(\"cuda\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "865ec1bb-9f71-4da6-8dd5-57e75a520745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = main_helper.create_model('timm/vit_tiny_patch16_224', pretrained=False, num_classes=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe13d102-16ee-4ff6-a960-2ca2b0934c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5,526,346\n",
      "Total parameters (M): 5.53M\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "injecting LoR\n",
      "Total parameters: 32,068,426\n",
      "Total parameters (M): 32.07M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32068426, 32.068426)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = main_helper.model_parser((-1,3,224,224), 10, config)\n",
    "regularizer =  main_helper.configure_model_experiment(model,config)\n",
    "main_helper.get_model_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ca89f4b-7946-43b9-91f7-bed0a77c5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloned_config = argparse.Namespace(\n",
    "    dataset='CIFAR10',\n",
    "    model='VIT_Tiny',\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    epoch=50,\n",
    "    technique='Truncate',\n",
    "    rectangular_mode=True,\n",
    "    target_rank=4,\n",
    "    lmbda=0.01,\n",
    "    depth_lorita=2,\n",
    "    weight_decay_alpha=0.00,\n",
    "    dropout=0.0,\n",
    "    threads=4,\n",
    "    augmentation=False,\n",
    "    target_modules=['Q', 'K', 'V','FC1','FC2'],\n",
    "    DEVICE = torch.device(\"cuda\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "424d3c2d-ce2b-41a6-bc2a-40626357c5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=4)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): LoRITa Module (in_features=192, out_features=768, N=4)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): LoRITa Module (in_features=768, out_features=192, N=4)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16467ed7-cf81-4566-b4da-9b174d74f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_vector = torch.randn(192)\n",
    "random_vector = random_vector.to(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2abeee7e-4c93-41b0-85b5-5d6bd3355d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lorita_result = model.blocks[0].attn.qkv(random_vector)\n",
    "fc1_lorita_result = model.blocks[0].mlp.fc1(random_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12819748-b1aa-427d-8bc5-26e593d6ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleansing LorITA scum\n",
      "['blocks.0.attn.qkv', 'blocks.0.attn.qkv.q_lorita', 'blocks.0.attn.qkv.k_lorita', 'blocks.0.attn.qkv.v_lorita', 'blocks.0.mlp.fc1', 'blocks.0.mlp.fc2', 'blocks.1.attn.qkv', 'blocks.1.attn.qkv.q_lorita', 'blocks.1.attn.qkv.k_lorita', 'blocks.1.attn.qkv.v_lorita', 'blocks.1.mlp.fc1', 'blocks.1.mlp.fc2', 'blocks.2.attn.qkv', 'blocks.2.attn.qkv.q_lorita', 'blocks.2.attn.qkv.k_lorita', 'blocks.2.attn.qkv.v_lorita', 'blocks.2.mlp.fc1', 'blocks.2.mlp.fc2', 'blocks.3.attn.qkv', 'blocks.3.attn.qkv.q_lorita', 'blocks.3.attn.qkv.k_lorita', 'blocks.3.attn.qkv.v_lorita', 'blocks.3.mlp.fc1', 'blocks.3.mlp.fc2', 'blocks.4.attn.qkv', 'blocks.4.attn.qkv.q_lorita', 'blocks.4.attn.qkv.k_lorita', 'blocks.4.attn.qkv.v_lorita', 'blocks.4.mlp.fc1', 'blocks.4.mlp.fc2', 'blocks.5.attn.qkv', 'blocks.5.attn.qkv.q_lorita', 'blocks.5.attn.qkv.k_lorita', 'blocks.5.attn.qkv.v_lorita', 'blocks.5.mlp.fc1', 'blocks.5.mlp.fc2', 'blocks.6.attn.qkv', 'blocks.6.attn.qkv.q_lorita', 'blocks.6.attn.qkv.k_lorita', 'blocks.6.attn.qkv.v_lorita', 'blocks.6.mlp.fc1', 'blocks.6.mlp.fc2', 'blocks.7.attn.qkv', 'blocks.7.attn.qkv.q_lorita', 'blocks.7.attn.qkv.k_lorita', 'blocks.7.attn.qkv.v_lorita', 'blocks.7.mlp.fc1', 'blocks.7.mlp.fc2', 'blocks.8.attn.qkv', 'blocks.8.attn.qkv.q_lorita', 'blocks.8.attn.qkv.k_lorita', 'blocks.8.attn.qkv.v_lorita', 'blocks.8.mlp.fc1', 'blocks.8.mlp.fc2', 'blocks.9.attn.qkv', 'blocks.9.attn.qkv.q_lorita', 'blocks.9.attn.qkv.k_lorita', 'blocks.9.attn.qkv.v_lorita', 'blocks.9.mlp.fc1', 'blocks.9.mlp.fc2', 'blocks.10.attn.qkv', 'blocks.10.attn.qkv.q_lorita', 'blocks.10.attn.qkv.k_lorita', 'blocks.10.attn.qkv.v_lorita', 'blocks.10.mlp.fc1', 'blocks.10.mlp.fc2', 'blocks.11.attn.qkv', 'blocks.11.attn.qkv.q_lorita', 'blocks.11.attn.qkv.k_lorita', 'blocks.11.attn.qkv.v_lorita', 'blocks.11.mlp.fc1', 'blocks.11.mlp.fc2']\n",
      "Replacing blocks.0.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.0.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.1.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.1.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.2.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.2.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.3.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.3.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.4.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.4.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.5.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.5.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.6.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.6.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.7.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.7.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.8.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.8.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.9.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.9.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.10.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.10.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "Replacing blocks.11.mlp.fc1 with Linear(in_features=192, out_features=768, bias=True)\n",
      "Replacing blocks.11.mlp.fc2 with Linear(in_features=768, out_features=192, bias=True)\n",
      "blocks.0.attn.qkv\n",
      "blocks.0.attn.qkv\n",
      "blocks.0.mlp.fc1\n",
      "blocks.0.mlp.fc1\n",
      "blocks.0.mlp.fc2\n",
      "blocks.0.mlp.fc2\n",
      "blocks.1.attn.qkv\n",
      "blocks.1.attn.qkv\n",
      "blocks.1.mlp.fc1\n",
      "blocks.1.mlp.fc1\n",
      "blocks.1.mlp.fc2\n",
      "blocks.1.mlp.fc2\n",
      "blocks.2.attn.qkv\n",
      "blocks.2.attn.qkv\n",
      "blocks.2.mlp.fc1\n",
      "blocks.2.mlp.fc1\n",
      "blocks.2.mlp.fc2\n",
      "blocks.2.mlp.fc2\n",
      "blocks.3.attn.qkv\n",
      "blocks.3.attn.qkv\n",
      "blocks.3.mlp.fc1\n",
      "blocks.3.mlp.fc1\n",
      "blocks.3.mlp.fc2\n",
      "blocks.3.mlp.fc2\n",
      "blocks.4.attn.qkv\n",
      "blocks.4.attn.qkv\n",
      "blocks.4.mlp.fc1\n",
      "blocks.4.mlp.fc1\n",
      "blocks.4.mlp.fc2\n",
      "blocks.4.mlp.fc2\n",
      "blocks.5.attn.qkv\n",
      "blocks.5.attn.qkv\n",
      "blocks.5.mlp.fc1\n",
      "blocks.5.mlp.fc1\n",
      "blocks.5.mlp.fc2\n",
      "blocks.5.mlp.fc2\n",
      "blocks.6.attn.qkv\n",
      "blocks.6.attn.qkv\n",
      "blocks.6.mlp.fc1\n",
      "blocks.6.mlp.fc1\n",
      "blocks.6.mlp.fc2\n",
      "blocks.6.mlp.fc2\n",
      "blocks.7.attn.qkv\n",
      "blocks.7.attn.qkv\n",
      "blocks.7.mlp.fc1\n",
      "blocks.7.mlp.fc1\n",
      "blocks.7.mlp.fc2\n",
      "blocks.7.mlp.fc2\n",
      "blocks.8.attn.qkv\n",
      "blocks.8.attn.qkv\n",
      "blocks.8.mlp.fc1\n",
      "blocks.8.mlp.fc1\n",
      "blocks.8.mlp.fc2\n",
      "blocks.8.mlp.fc2\n",
      "blocks.9.attn.qkv\n",
      "blocks.9.attn.qkv\n",
      "blocks.9.mlp.fc1\n",
      "blocks.9.mlp.fc1\n",
      "blocks.9.mlp.fc2\n",
      "blocks.9.mlp.fc2\n",
      "blocks.10.attn.qkv\n",
      "blocks.10.attn.qkv\n",
      "blocks.10.mlp.fc1\n",
      "blocks.10.mlp.fc1\n",
      "blocks.10.mlp.fc2\n",
      "blocks.10.mlp.fc2\n",
      "blocks.11.attn.qkv\n",
      "blocks.11.attn.qkv\n",
      "blocks.11.mlp.fc1\n",
      "blocks.11.mlp.fc1\n",
      "blocks.11.mlp.fc2\n",
      "blocks.11.mlp.fc2\n",
      "Total parameters: 5,526,346\n",
      "Total parameters (M): 5.53M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5526346, 5.526346)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularizer =  main_helper.configure_model_experiment(model,cloned_config)\n",
    "main_helper.get_model_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "642d460c-883d-45b6-952e-1fbda8a5dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify_result = model.blocks[0].attn.qkv(random_vector)\n",
    "fc1_simplify_result = model.blocks[0].mlp.fc1(random_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e43a578-66c6-4d62-9a48-18418c89e983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.4703e-08, -3.7253e-08, -2.2352e-08, -5.2154e-08, -2.2352e-08,\n",
       "        -5.2154e-08,  5.9605e-08, -1.4901e-08, -1.4901e-08,  3.7253e-08,\n",
       "        -1.4901e-08,  5.9605e-08, -5.2154e-08, -2.2352e-08, -1.6764e-08,\n",
       "         3.7253e-09,  5.9605e-08, -1.8626e-09,  1.8626e-08, -2.2352e-08,\n",
       "        -7.4506e-09,  7.4506e-08, -5.2154e-08,  0.0000e+00, -5.9605e-08,\n",
       "         1.3039e-08, -5.2154e-08,  2.2352e-08,  2.9802e-08, -1.4901e-08,\n",
       "         4.4703e-08,  0.0000e+00, -1.4901e-08, -1.4901e-08,  6.7055e-08,\n",
       "         2.9802e-08,  1.1176e-08, -3.1665e-08, -2.9802e-08,  2.2352e-08,\n",
       "        -1.4901e-08, -1.4901e-08, -4.4703e-08,  0.0000e+00,  2.9802e-08,\n",
       "         7.4506e-09,  2.2352e-08,  4.4703e-08, -4.8429e-08,  0.0000e+00,\n",
       "         1.4901e-08,  7.4506e-09, -2.9802e-08, -8.9407e-08,  1.1176e-08,\n",
       "         3.7253e-09, -2.9802e-08, -4.4703e-08,  0.0000e+00, -3.7253e-09,\n",
       "         5.5879e-08, -1.4901e-08, -4.0978e-08, -2.2352e-08, -5.9605e-08,\n",
       "        -7.4506e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4901e-08,\n",
       "         1.4901e-08,  1.8626e-08,  3.7253e-08, -5.9605e-08,  1.4901e-08,\n",
       "        -4.6566e-08,  6.7055e-08,  7.4506e-09, -5.9605e-08, -3.7253e-09,\n",
       "         2.9802e-08,  1.4901e-08, -1.4901e-08, -1.4901e-08,  0.0000e+00,\n",
       "         5.9605e-08,  1.4901e-08,  6.7055e-08,  2.2352e-08, -2.9802e-08,\n",
       "         1.8626e-08,  0.0000e+00,  5.9605e-08,  1.4901e-08,  0.0000e+00,\n",
       "        -4.0978e-08,  4.4703e-08,  1.4901e-08, -3.7253e-09, -2.9802e-08,\n",
       "        -5.5879e-09,  2.2352e-08,  2.9802e-08,  5.2154e-08, -7.4506e-08,\n",
       "         2.7940e-08, -4.8429e-08,  5.2154e-08,  2.9802e-08,  1.4901e-08,\n",
       "         7.4506e-09,  2.9802e-08, -2.6077e-08, -2.9802e-08,  2.9802e-08,\n",
       "        -3.7253e-08, -1.4901e-08, -7.8231e-08, -5.9605e-08,  1.4901e-08,\n",
       "         7.4506e-08,  1.4901e-08, -4.4703e-08,  2.9802e-08,  8.9407e-08,\n",
       "         0.0000e+00,  1.4901e-08, -5.9605e-08, -1.8626e-08,  8.9407e-08,\n",
       "        -3.7253e-08,  5.5879e-08,  4.0978e-08, -2.9802e-08, -1.4901e-08,\n",
       "         0.0000e+00, -2.9802e-08,  0.0000e+00, -1.8626e-08, -1.4901e-08,\n",
       "        -4.4703e-08,  7.0781e-08, -4.0978e-08, -4.4703e-08, -7.4506e-09,\n",
       "        -1.1176e-08,  5.9605e-08, -4.4703e-08,  1.4901e-08,  7.4506e-09,\n",
       "         0.0000e+00,  2.6077e-08, -7.4506e-09,  2.9802e-08, -2.2352e-08,\n",
       "        -7.4506e-09, -5.5879e-08,  1.1176e-08, -2.2352e-08, -1.4901e-08,\n",
       "        -1.4901e-08, -1.4901e-08,  7.4506e-09,  2.6077e-08,  5.2154e-08,\n",
       "        -3.3528e-08, -2.2352e-08, -2.2352e-08, -2.9802e-08,  0.0000e+00,\n",
       "         5.0291e-08,  0.0000e+00,  0.0000e+00, -3.7253e-09,  2.9802e-08,\n",
       "         3.1665e-08,  0.0000e+00,  3.3528e-08, -2.2352e-08, -4.4703e-08,\n",
       "        -1.4901e-08, -1.4901e-08, -2.9802e-08, -3.7253e-08, -2.2352e-08,\n",
       "         2.9802e-08,  0.0000e+00,  2.6077e-08,  7.4506e-08, -3.7253e-08,\n",
       "         0.0000e+00, -2.9802e-08, -4.4703e-08,  2.9802e-08,  2.9802e-08,\n",
       "        -7.4506e-09,  2.9802e-08, -7.0781e-08, -1.4901e-08, -3.7253e-09,\n",
       "        -5.2154e-08,  2.6077e-08, -2.9802e-08,  2.2352e-08,  2.9802e-08,\n",
       "         5.5879e-08,  4.4703e-08, -5.9605e-08,  0.0000e+00,  2.9802e-08,\n",
       "        -5.9605e-08, -7.4506e-08, -4.4703e-08, -5.9605e-08, -7.4506e-09,\n",
       "         2.9802e-08,  1.8626e-08,  7.4506e-09, -2.9802e-08,  5.5879e-08,\n",
       "        -2.2352e-08,  1.4901e-08,  1.4901e-08,  0.0000e+00, -1.4901e-08,\n",
       "         5.4017e-08, -2.9802e-08, -5.2154e-08, -1.4901e-08, -1.4901e-08,\n",
       "        -1.1176e-08, -2.9802e-08,  2.9802e-08,  2.2352e-08, -2.2352e-08,\n",
       "         3.7253e-08, -1.8626e-08, -1.4901e-08,  2.9802e-08, -1.4901e-08,\n",
       "        -7.4506e-09,  1.4901e-08, -1.4901e-08, -8.9407e-08, -3.7253e-08,\n",
       "         1.6764e-08,  2.9802e-08,  2.2352e-08,  3.3528e-08, -3.7253e-08,\n",
       "         1.4901e-08,  0.0000e+00,  3.2596e-08, -3.7253e-08,  1.4901e-08,\n",
       "        -2.9802e-08, -7.8231e-08,  3.7253e-08, -1.4901e-08, -1.1176e-08,\n",
       "         7.4506e-09, -1.4901e-08,  2.9802e-08,  2.2352e-08,  2.9802e-08,\n",
       "         5.9605e-08, -1.4901e-08,  3.3528e-08,  0.0000e+00,  2.9802e-08,\n",
       "        -7.4506e-09,  2.9802e-08,  8.9407e-08,  1.8626e-08, -7.4506e-09,\n",
       "         3.7253e-09, -2.9802e-08,  3.7253e-08, -2.9802e-08,  2.9802e-08,\n",
       "         4.0978e-08,  4.0978e-08, -2.9802e-08, -2.9802e-08,  7.4506e-09,\n",
       "         4.4703e-08, -5.2154e-08, -1.4901e-08,  0.0000e+00,  5.9605e-08,\n",
       "         3.7253e-08, -2.9802e-08, -2.2352e-08, -3.7253e-08, -2.9802e-08,\n",
       "         1.4901e-08,  2.2352e-08,  2.2352e-08,  0.0000e+00,  2.2352e-08,\n",
       "         0.0000e+00,  0.0000e+00,  2.7940e-09, -7.4506e-09, -2.9802e-08,\n",
       "         5.2154e-08,  0.0000e+00,  5.5879e-08,  9.3132e-09,  7.4506e-08,\n",
       "         3.7253e-09, -1.4901e-08,  2.9802e-08, -1.4901e-08,  3.7253e-08,\n",
       "         0.0000e+00,  2.2352e-08,  0.0000e+00,  0.0000e+00,  2.9802e-08,\n",
       "        -9.6858e-08,  4.4703e-08, -7.4506e-09,  1.4901e-08, -4.4703e-08,\n",
       "         7.4506e-09,  5.9605e-08,  1.4901e-08,  3.7253e-08, -5.9605e-08,\n",
       "         1.4901e-08,  0.0000e+00,  7.4506e-09, -2.9802e-08, -5.9605e-08,\n",
       "        -4.8429e-08, -6.3330e-08, -3.7253e-08,  2.9802e-08, -2.2352e-08,\n",
       "        -3.7253e-08,  1.8626e-08,  7.4506e-08, -4.4703e-08,  6.3330e-08,\n",
       "        -2.9802e-08, -1.4901e-08, -2.9802e-08,  3.7253e-09,  5.9605e-08,\n",
       "         5.9605e-08, -2.2352e-08,  2.9802e-08, -1.4901e-08,  1.4901e-08,\n",
       "        -2.9802e-08,  1.4901e-08, -1.4901e-08,  2.9802e-08,  1.4901e-08,\n",
       "        -2.0489e-08,  0.0000e+00,  3.7253e-08,  5.9605e-08,  0.0000e+00,\n",
       "        -1.4901e-08,  1.4901e-08, -3.7253e-08,  4.4703e-08,  1.4901e-08,\n",
       "         0.0000e+00, -4.4703e-08,  0.0000e+00, -1.8626e-08, -3.7253e-08,\n",
       "        -2.9802e-08,  0.0000e+00, -1.8626e-09,  0.0000e+00, -2.9802e-08,\n",
       "         0.0000e+00,  1.4901e-08, -2.9802e-08, -2.9802e-08,  0.0000e+00,\n",
       "         4.4703e-08, -1.4901e-08, -9.6858e-08,  7.4506e-08, -3.7253e-08,\n",
       "         1.4901e-08,  0.0000e+00, -2.9802e-08,  1.4901e-08, -3.3528e-08,\n",
       "        -8.9407e-08,  1.4901e-08,  1.4901e-08, -3.7253e-09,  5.2154e-08,\n",
       "         2.9802e-08,  8.9407e-08, -7.4506e-09,  2.2352e-08, -2.9802e-08,\n",
       "         2.9802e-08, -3.7253e-08,  4.4703e-08, -1.4901e-08, -1.4901e-08,\n",
       "        -1.1176e-08,  5.9605e-08, -1.4901e-08,  2.9802e-08,  3.7253e-09,\n",
       "         8.3819e-08,  0.0000e+00, -5.0291e-08, -2.9802e-08,  5.5879e-09,\n",
       "         0.0000e+00,  2.4214e-08,  7.4506e-09,  1.4901e-08,  6.7055e-08,\n",
       "         0.0000e+00,  2.2352e-08, -4.0978e-08,  0.0000e+00, -1.4901e-08,\n",
       "        -5.9605e-08,  0.0000e+00,  0.0000e+00,  1.4901e-08, -5.9605e-08,\n",
       "         7.4506e-09,  6.7055e-08,  0.0000e+00, -4.4703e-08, -7.4506e-09,\n",
       "         8.9407e-08,  3.7253e-08,  5.2154e-08, -9.3132e-08,  5.2154e-08,\n",
       "         4.4703e-08,  2.9802e-08,  2.9802e-08, -7.4506e-09, -4.0978e-08,\n",
       "         4.4703e-08,  2.6077e-08, -4.8429e-08, -5.9605e-08,  1.4901e-08,\n",
       "         1.1176e-08, -5.9605e-08, -3.7253e-09,  0.0000e+00, -2.0489e-08,\n",
       "         0.0000e+00, -7.4506e-09,  0.0000e+00, -5.5879e-08, -5.9605e-08,\n",
       "        -2.6077e-08, -2.2352e-08, -8.1956e-08, -4.0978e-08,  8.1956e-08,\n",
       "        -4.4703e-08, -7.4506e-09,  2.2352e-08,  5.9605e-08, -2.2352e-08,\n",
       "         7.4506e-09, -3.7253e-08,  7.4506e-09,  0.0000e+00, -2.2352e-08,\n",
       "        -2.9802e-08,  2.9802e-08, -3.7253e-08, -2.9802e-08,  7.4506e-09,\n",
       "         2.9802e-08,  0.0000e+00, -2.9802e-08,  0.0000e+00, -4.4703e-08,\n",
       "        -5.9605e-08, -4.4703e-08, -2.9802e-08, -1.4901e-08, -5.2154e-08,\n",
       "        -1.4901e-08,  0.0000e+00,  2.9802e-08, -1.4901e-08,  4.0978e-08,\n",
       "         5.2154e-08, -5.9605e-08, -4.4703e-08,  7.4506e-09, -1.4901e-08,\n",
       "         2.9802e-08,  2.2352e-08,  1.4901e-08,  1.4901e-08, -2.9802e-08,\n",
       "         0.0000e+00,  7.4506e-08,  0.0000e+00,  3.7253e-08,  0.0000e+00,\n",
       "         4.4703e-08,  5.2154e-08,  2.9802e-08, -7.4506e-09, -2.9802e-08,\n",
       "         2.2352e-08,  2.9802e-08,  0.0000e+00, -6.7055e-08, -5.9605e-08,\n",
       "         2.9802e-08,  3.7253e-08, -1.4901e-08, -1.8626e-08, -7.4506e-09,\n",
       "         1.4901e-08, -1.4901e-08, -1.4901e-08,  2.9802e-08,  7.4506e-08,\n",
       "         8.7544e-08, -2.9802e-08, -3.3528e-08,  2.9802e-08,  3.7253e-09,\n",
       "        -4.4703e-08, -4.4703e-08,  0.0000e+00,  2.9802e-08, -5.9605e-08,\n",
       "         0.0000e+00, -1.0431e-07,  3.7253e-09,  3.7253e-08,  1.4901e-08,\n",
       "        -5.2154e-08, -2.9802e-08, -5.9605e-08,  0.0000e+00,  7.4506e-09,\n",
       "         7.4506e-08,  4.4703e-08, -7.4506e-09, -7.4506e-08, -5.2154e-08,\n",
       "         5.9605e-08,  8.9407e-08,  6.7055e-08, -2.2352e-08, -1.4901e-08,\n",
       "         0.0000e+00,  2.2352e-08,  3.7253e-08, -7.4506e-09,  2.9802e-08,\n",
       "         2.9802e-08,  1.4901e-08, -3.7253e-08,  1.4901e-08,  2.9802e-08,\n",
       "        -7.4506e-09], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lorita_result-simplify_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9597f5f4-3b7b-40e1-9444-911b4f2a36b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4901e-08, -1.8626e-08, -2.2352e-08,  5.9605e-08,  0.0000e+00,\n",
       "        -2.9802e-08,  0.0000e+00, -5.2154e-08, -4.8429e-08, -5.2154e-08,\n",
       "         7.4506e-09,  7.4506e-09,  0.0000e+00,  0.0000e+00,  2.9802e-08,\n",
       "        -2.9802e-08, -3.7253e-09, -1.4901e-08, -2.9802e-08,  0.0000e+00,\n",
       "         1.4901e-08, -5.5879e-09, -1.8626e-08, -2.9802e-08, -2.9802e-08,\n",
       "         4.4703e-08, -6.7055e-08, -4.4703e-08, -7.4506e-08, -1.8626e-08,\n",
       "        -5.2154e-08, -4.4703e-08,  1.4901e-08,  0.0000e+00,  0.0000e+00,\n",
       "        -4.0978e-08, -2.9802e-08, -1.4901e-08,  0.0000e+00,  0.0000e+00,\n",
       "        -8.9407e-08,  2.6077e-08, -2.9802e-08,  2.9802e-08,  2.9802e-08,\n",
       "         0.0000e+00, -4.4703e-08, -2.2352e-08, -2.2352e-08,  0.0000e+00,\n",
       "        -7.4506e-09, -2.2352e-08, -2.9802e-08,  1.4901e-08,  2.2352e-08,\n",
       "         2.9802e-08,  7.4506e-09,  7.4506e-08, -3.7253e-09, -1.4901e-08,\n",
       "        -1.4901e-08,  4.0978e-08, -1.8626e-08,  5.9605e-08,  0.0000e+00,\n",
       "         5.9605e-08, -7.4506e-09,  7.4506e-09,  0.0000e+00, -2.9802e-08,\n",
       "         7.4506e-08,  2.6077e-08,  7.4506e-09,  2.9802e-08, -2.9802e-08,\n",
       "        -8.9407e-08,  3.7253e-09, -1.4901e-08,  0.0000e+00,  1.4901e-08,\n",
       "         0.0000e+00, -5.2154e-08,  7.4506e-09,  2.6077e-08,  0.0000e+00,\n",
       "         0.0000e+00,  2.9802e-08, -2.9802e-08,  0.0000e+00, -4.4703e-08,\n",
       "        -1.1176e-08,  4.4703e-08, -2.9802e-08, -3.7253e-09,  7.4506e-09,\n",
       "         1.4901e-08,  2.2352e-08, -4.4703e-08, -2.9802e-08, -5.2154e-08,\n",
       "        -1.4901e-08, -3.7253e-08,  0.0000e+00,  4.6566e-08, -1.4901e-08,\n",
       "         2.6077e-08,  1.4901e-08,  0.0000e+00,  2.6077e-08,  4.4703e-08,\n",
       "         7.4506e-09,  1.4901e-08, -6.7055e-08,  7.8231e-08,  3.7253e-08,\n",
       "        -5.2154e-08, -1.4901e-08,  7.4506e-09,  2.9802e-08,  5.9605e-08,\n",
       "        -5.9605e-08,  2.9802e-08,  4.4703e-08,  1.4901e-08,  2.2352e-08,\n",
       "         0.0000e+00, -7.4506e-09, -1.4901e-08, -2.9802e-08,  7.4506e-09,\n",
       "         1.4901e-08,  0.0000e+00, -7.4506e-09,  7.4506e-09,  3.7253e-08,\n",
       "        -4.4703e-08, -7.4506e-09, -3.7253e-09,  0.0000e+00, -7.4506e-09,\n",
       "         2.2352e-08, -2.9802e-08, -4.4703e-08,  0.0000e+00, -1.4901e-08,\n",
       "         2.9802e-08,  5.2154e-08, -2.9802e-08,  2.9802e-08,  0.0000e+00,\n",
       "         2.9802e-08,  0.0000e+00,  3.7253e-08, -3.7253e-09,  0.0000e+00,\n",
       "         7.4506e-09,  2.9802e-08, -3.7253e-08,  1.4901e-08, -2.2352e-08,\n",
       "        -7.4506e-09,  0.0000e+00, -5.2154e-08, -3.7253e-08,  1.4901e-08,\n",
       "        -1.4901e-08, -5.9605e-08,  7.4506e-09,  3.7253e-08, -2.9802e-08,\n",
       "        -3.7253e-08, -2.9802e-08,  1.4901e-08,  2.2352e-08, -4.4703e-08,\n",
       "        -4.4703e-08,  7.4506e-09, -2.9802e-08, -2.9802e-08, -7.4506e-09,\n",
       "         2.2352e-08, -9.3132e-09, -1.4901e-08, -2.9802e-08,  0.0000e+00,\n",
       "        -1.4901e-08, -2.2352e-08, -1.1176e-08,  3.7253e-08,  1.4901e-08,\n",
       "        -5.9605e-08, -5.5879e-08, -4.4703e-08,  1.1176e-08,  0.0000e+00,\n",
       "         0.0000e+00,  5.9605e-08, -7.4506e-09,  2.2352e-08, -4.1910e-08,\n",
       "         5.9605e-08, -4.4703e-08, -4.4703e-08, -2.9802e-08,  7.4506e-08,\n",
       "         1.3039e-08,  0.0000e+00,  7.4506e-09,  3.7253e-09, -5.9605e-08,\n",
       "        -3.7253e-09,  1.8626e-08, -1.4901e-08, -1.4901e-08, -1.4901e-08,\n",
       "        -1.4901e-08, -7.4506e-09, -7.4506e-09,  0.0000e+00,  1.4901e-08,\n",
       "         2.9802e-08,  0.0000e+00,  6.7055e-08,  2.9802e-08,  2.2352e-08,\n",
       "        -1.4901e-08,  0.0000e+00,  2.2352e-08,  5.2154e-08,  2.2352e-08,\n",
       "        -3.9116e-08, -7.4506e-09,  0.0000e+00, -1.8626e-08,  4.4703e-08,\n",
       "        -1.1176e-08,  2.9802e-08, -1.4901e-08,  0.0000e+00,  0.0000e+00,\n",
       "        -2.9802e-08, -2.9802e-08,  1.4901e-08, -3.3528e-08,  0.0000e+00,\n",
       "         1.4901e-08,  1.8626e-08,  1.4901e-08,  0.0000e+00, -3.3528e-08,\n",
       "        -1.4901e-08,  1.4901e-08, -2.2352e-08,  7.4506e-09,  1.4901e-08,\n",
       "         0.0000e+00, -2.9802e-08,  4.4703e-08,  0.0000e+00, -2.2352e-08,\n",
       "        -1.4901e-08, -2.2352e-08, -5.9605e-08,  3.7253e-09,  0.0000e+00,\n",
       "        -2.4214e-08,  1.4901e-08,  7.4506e-09, -7.4506e-09, -1.1176e-08,\n",
       "         1.4901e-08, -1.4901e-08,  0.0000e+00, -4.4703e-08, -1.4901e-08,\n",
       "        -2.9802e-08,  7.4506e-09, -5.9605e-08,  7.4506e-09,  1.4901e-08,\n",
       "         1.4901e-08,  0.0000e+00, -3.7253e-08,  2.9802e-08,  2.9802e-08,\n",
       "        -5.5879e-08,  4.4703e-08, -3.7253e-08,  2.9802e-08, -2.9802e-08,\n",
       "        -2.2352e-08,  1.8626e-08,  1.4901e-08,  0.0000e+00,  5.5879e-08,\n",
       "        -1.4901e-08, -1.8626e-08, -1.4901e-08,  0.0000e+00, -2.9802e-08,\n",
       "        -7.4506e-09, -5.5879e-08, -2.9802e-08,  2.2352e-08,  2.2352e-08,\n",
       "         2.9802e-08,  6.7055e-08,  7.4506e-09, -6.3330e-08,  1.4901e-08,\n",
       "         3.7253e-09, -3.7253e-08,  2.9802e-08, -1.4901e-08,  2.9802e-08,\n",
       "        -1.4901e-08, -4.0978e-08, -3.7253e-08,  7.4506e-08,  0.0000e+00,\n",
       "         0.0000e+00, -1.4901e-08,  7.4506e-09,  2.2352e-08, -1.8626e-08,\n",
       "         1.4901e-08, -2.2352e-08,  5.2154e-08,  0.0000e+00,  2.9802e-08,\n",
       "         0.0000e+00, -2.9802e-08, -4.4703e-08,  1.1176e-08,  0.0000e+00,\n",
       "         3.7253e-09,  0.0000e+00,  2.9802e-08, -3.7253e-09,  3.3528e-08,\n",
       "         1.4901e-08,  5.9605e-08, -5.9605e-08,  1.4901e-08,  8.9407e-08,\n",
       "         5.9605e-08, -1.4901e-08,  2.2352e-08,  2.2352e-08, -2.2352e-08,\n",
       "         3.3528e-08,  7.4506e-09, -4.4703e-08, -1.4901e-08, -2.9802e-08,\n",
       "        -1.4901e-08, -2.9802e-08,  3.7253e-08, -1.1176e-08, -2.9802e-08,\n",
       "         1.4901e-08, -1.4901e-08,  1.4901e-08, -2.6077e-08,  1.0058e-07,\n",
       "         0.0000e+00,  0.0000e+00,  5.5879e-08,  1.4901e-08,  0.0000e+00,\n",
       "        -1.4901e-08,  2.6077e-08, -1.1176e-08,  1.8626e-08, -5.9605e-08,\n",
       "        -2.2352e-08,  0.0000e+00, -2.2352e-08,  0.0000e+00, -4.8429e-08,\n",
       "        -6.7055e-08,  0.0000e+00, -1.1176e-08, -1.4901e-08,  0.0000e+00,\n",
       "        -4.4703e-08, -3.3528e-08,  4.4703e-08,  7.4506e-09,  0.0000e+00,\n",
       "         0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00, -4.4703e-08,\n",
       "         1.4901e-08, -4.4703e-08, -2.6077e-08,  2.2352e-08, -5.2154e-08,\n",
       "         0.0000e+00,  3.3528e-08,  4.4703e-08, -3.7253e-08,  0.0000e+00,\n",
       "         2.9802e-08, -5.9605e-08,  2.2352e-08, -3.9116e-08,  0.0000e+00,\n",
       "        -2.9802e-08, -7.4506e-09,  0.0000e+00, -2.9802e-08, -1.4901e-08,\n",
       "         7.4506e-09,  1.4901e-08, -1.1176e-08,  1.4901e-08,  7.4506e-09,\n",
       "         0.0000e+00, -1.4901e-08,  2.2352e-08,  2.2352e-08,  7.4506e-08,\n",
       "        -3.7253e-08,  7.4506e-09,  0.0000e+00,  2.9802e-08, -3.7253e-09,\n",
       "         3.7253e-08,  2.9802e-08,  4.6566e-08,  8.9407e-08,  0.0000e+00,\n",
       "         0.0000e+00,  1.4901e-08,  5.9605e-08,  2.9802e-08,  0.0000e+00,\n",
       "        -4.4703e-08,  7.4506e-08, -2.9802e-08, -5.4017e-08,  1.4901e-08,\n",
       "        -7.4506e-08,  6.7055e-08, -1.4901e-08, -7.4506e-09, -2.9802e-08,\n",
       "        -7.4506e-09, -3.3528e-08, -1.4901e-08,  3.3528e-08,  2.9802e-08,\n",
       "         5.9605e-08,  3.3528e-08, -4.4703e-08, -2.9802e-08, -2.9802e-08,\n",
       "         2.2352e-08,  2.9802e-08,  3.7253e-08,  2.9802e-08,  2.9802e-08,\n",
       "        -1.4901e-08,  2.9802e-08,  7.4506e-08,  7.4506e-09, -3.3528e-08,\n",
       "         0.0000e+00,  2.9802e-08,  2.9802e-08,  1.4901e-08, -1.4901e-08,\n",
       "         1.4901e-08,  1.8626e-08,  1.4901e-08,  4.4703e-08, -3.7253e-08,\n",
       "         2.6077e-08,  0.0000e+00,  3.7253e-09, -2.2352e-08, -2.2352e-08,\n",
       "        -1.4901e-08,  1.1176e-08,  1.4901e-08, -7.4506e-09, -2.9802e-08,\n",
       "         0.0000e+00,  7.4506e-09,  2.9802e-08, -4.4703e-08, -2.9802e-08,\n",
       "         3.7253e-08, -7.4506e-09, -3.7253e-08,  0.0000e+00,  0.0000e+00,\n",
       "         2.9802e-08,  0.0000e+00,  2.6077e-08,  2.2352e-08, -4.4703e-08,\n",
       "         2.2352e-08, -1.4901e-08, -1.4901e-08,  5.7742e-08,  0.0000e+00,\n",
       "        -2.2352e-08, -1.1176e-08, -7.4506e-09,  7.4506e-09,  0.0000e+00,\n",
       "         2.9802e-08,  0.0000e+00,  5.9605e-08, -2.2352e-08, -7.4506e-08,\n",
       "        -2.9802e-08,  0.0000e+00, -4.0978e-08, -3.7253e-08, -1.4901e-08,\n",
       "         3.3528e-08, -2.9802e-08, -7.4506e-09,  0.0000e+00, -2.2352e-08,\n",
       "        -1.4901e-08,  0.0000e+00,  7.4506e-09,  7.4506e-09, -3.7253e-08,\n",
       "        -1.4901e-08,  0.0000e+00, -2.9802e-08, -2.9802e-08,  0.0000e+00,\n",
       "         4.4703e-08,  1.4901e-08,  1.8626e-08, -7.4506e-09,  7.4506e-08,\n",
       "         1.1176e-08, -7.4506e-09,  5.9605e-08,  1.4901e-08, -1.4901e-08,\n",
       "        -4.4703e-08, -1.8626e-08,  7.4506e-09, -6.7055e-08, -2.2352e-08,\n",
       "         7.4506e-09,  0.0000e+00, -4.0978e-08,  3.7253e-09,  1.4901e-08,\n",
       "        -3.3528e-08, -2.6077e-08, -3.7253e-08, -5.9605e-08,  1.4901e-08,\n",
       "         4.4703e-08,  0.0000e+00,  5.9605e-08,  4.4703e-08, -8.5682e-08,\n",
       "        -7.4506e-09,  0.0000e+00,  7.4506e-09,  5.9605e-08,  0.0000e+00,\n",
       "         2.2352e-08, -2.9802e-08, -6.7055e-08,  1.4901e-08, -1.4901e-08,\n",
       "        -2.9802e-08, -2.9802e-08,  2.9802e-08,  0.0000e+00, -3.7253e-08,\n",
       "        -2.9802e-08, -1.4901e-08,  0.0000e+00,  7.4506e-09, -3.7253e-08,\n",
       "         3.7253e-09,  0.0000e+00,  1.4901e-08,  1.4901e-08,  7.4506e-09,\n",
       "         0.0000e+00,  5.2154e-08,  0.0000e+00,  2.9802e-08,  2.2352e-08,\n",
       "        -4.8429e-08,  0.0000e+00, -1.4901e-08, -7.4506e-09, -4.2841e-08,\n",
       "        -1.4901e-08, -1.4901e-08,  2.6077e-08,  3.7253e-08,  4.4703e-08,\n",
       "        -2.9802e-08, -4.4703e-08,  0.0000e+00,  2.2352e-08, -2.2352e-08,\n",
       "        -1.4901e-08, -6.3330e-08,  4.4703e-08, -3.7253e-08,  2.9802e-08,\n",
       "        -1.1176e-08,  7.4506e-09,  7.4506e-09, -1.4901e-08,  2.9802e-08,\n",
       "         0.0000e+00,  5.9605e-08, -3.7253e-09, -3.7253e-08, -3.7253e-08,\n",
       "         5.9605e-08,  1.4901e-08, -7.4506e-09, -2.9802e-08, -3.3528e-08,\n",
       "        -3.7253e-09,  2.9802e-08, -5.2154e-08,  7.4506e-09,  0.0000e+00,\n",
       "        -2.9802e-08,  0.0000e+00,  1.1176e-08, -3.7253e-08,  4.4703e-08,\n",
       "        -1.1176e-08, -2.9802e-08,  0.0000e+00,  4.0978e-08, -2.9802e-08,\n",
       "         0.0000e+00, -4.8429e-08, -2.9802e-08,  1.4901e-08,  2.2352e-08,\n",
       "        -2.9802e-08, -2.9802e-08,  7.4506e-09,  1.4901e-08,  7.4506e-09,\n",
       "        -1.4901e-08, -7.4506e-09,  2.9802e-08,  7.4506e-09,  1.4901e-08,\n",
       "         2.6077e-08,  1.4901e-08, -5.9605e-08, -3.7253e-09, -7.4506e-09,\n",
       "        -3.7253e-09, -4.4703e-08, -4.4703e-08, -2.6077e-08,  1.4901e-08,\n",
       "         0.0000e+00,  2.9802e-08,  0.0000e+00,  1.4901e-08,  2.9802e-08,\n",
       "         5.9605e-08, -7.4506e-09,  0.0000e+00,  3.7253e-09, -2.9802e-08,\n",
       "         3.7253e-08,  2.9802e-08, -4.4703e-08,  0.0000e+00,  3.7253e-09,\n",
       "         7.4506e-09,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
       "         1.8626e-08,  2.2352e-08, -7.4506e-09,  7.4506e-09,  9.6858e-08,\n",
       "        -3.3528e-08, -2.9802e-08,  2.9802e-08,  2.2352e-08,  7.4506e-09,\n",
       "        -2.9802e-08,  1.4901e-08, -4.4703e-08, -2.9802e-08,  0.0000e+00,\n",
       "         1.4901e-08,  4.4703e-08, -2.9802e-08,  1.4901e-08,  5.5879e-08,\n",
       "         2.2352e-08,  0.0000e+00,  4.8429e-08, -1.4901e-08,  2.6077e-08,\n",
       "        -1.4901e-08,  5.9605e-08,  2.2352e-08,  3.7253e-08,  1.4901e-08,\n",
       "         2.9802e-08,  2.9802e-08, -3.7253e-08, -1.8626e-08, -1.1176e-08,\n",
       "         0.0000e+00, -5.9605e-08, -3.7253e-09, -1.4901e-08,  7.4506e-09,\n",
       "        -2.6077e-08,  4.4703e-08, -1.8626e-08, -5.9605e-08, -2.9802e-08,\n",
       "        -1.4901e-08, -1.8626e-08,  0.0000e+00,  7.4506e-09,  1.4901e-08,\n",
       "         2.4214e-08,  2.9802e-08, -7.4506e-09, -2.9802e-08,  1.4901e-08,\n",
       "        -1.4901e-08,  1.4901e-08, -2.9802e-08,  1.4901e-08, -3.7253e-09,\n",
       "         2.0489e-08,  5.9605e-08, -3.7253e-09, -2.9802e-08,  7.4506e-09,\n",
       "        -5.9605e-08, -1.8626e-08,  4.4703e-08,  0.0000e+00,  0.0000e+00,\n",
       "         3.7253e-09, -7.4506e-09,  1.1176e-08], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1_lorita_result-fc1_simplify_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e51d2e18-8124-4415-af1f-6a172cec67bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e998d10-79e8-4215-9b22-8ba716f95e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
