{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afd3888-0ed7-42d8-b271-553755b145d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enguye17\\PycharmProjects\\QuaRS\\new_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from timm import create_model\n",
    "from timm.models import ResNet\n",
    "import torch\n",
    "import main_helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a450954e-64a8-4101-9782-615dce4cc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "config = argparse.Namespace(\n",
    "    dataset='CIFAR10',\n",
    "    model='VIT_Tiny',\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    epoch=50,\n",
    "    technique='LoRITa',\n",
    "    rectangular_mode=True,\n",
    "    target_rank=4,\n",
    "    lmbda=0.01,\n",
    "    depth_lorita=1,\n",
    "    weight_decay_alpha=0.00,\n",
    "    dropout=0.0,\n",
    "    threads=4,\n",
    "    augmentation=False,\n",
    "    target_modules=['Q', 'K', 'V'],\n",
    "    DEVICE = torch.device(\"cuda\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7997e2-7605-4a9c-b75f-9735d545f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "injecting LoR\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): LoRITaQKV(\n",
      "          (q_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (k_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "          (v_lorita): LoRITa Module (in_features=192, out_features=192, N=1)\n",
      "        )\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = main_helper.model_parser((-1,3,224,224), 10, config)\n",
    "regularizer =  main_helper.configure_model_experiment(model,config)\n",
    "main_helper.get_model_memory(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac10e8c2-842f-4166-bda3-9619cc62bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloned_config = argparse.Namespace(\n",
    "    dataset='CIFAR10',\n",
    "    model='VIT_Tiny',\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    epoch=50,\n",
    "    technique='Truncate',\n",
    "    rectangular_mode=True,\n",
    "    target_rank=0.1,\n",
    "    lmbda=0.01,\n",
    "    depth_lorita=2,\n",
    "    weight_decay_alpha=0.00,\n",
    "    dropout=0.0,\n",
    "    threads=4,\n",
    "    augmentation=False,\n",
    "    target_modules=['Q', 'K', 'V'],\n",
    "    DEVICE = torch.device(\"cuda\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e65f9f8-73f4-41da-9321-50aa6d5c6178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRITa Modules: ['blocks.0.attn.qkv', 'blocks.0.attn.qkv.q_lorita', 'blocks.0.attn.qkv.k_lorita', 'blocks.0.attn.qkv.v_lorita', 'blocks.1.attn.qkv', 'blocks.1.attn.qkv.q_lorita', 'blocks.1.attn.qkv.k_lorita', 'blocks.1.attn.qkv.v_lorita', 'blocks.2.attn.qkv', 'blocks.2.attn.qkv.q_lorita', 'blocks.2.attn.qkv.k_lorita', 'blocks.2.attn.qkv.v_lorita', 'blocks.3.attn.qkv', 'blocks.3.attn.qkv.q_lorita', 'blocks.3.attn.qkv.k_lorita', 'blocks.3.attn.qkv.v_lorita', 'blocks.4.attn.qkv', 'blocks.4.attn.qkv.q_lorita', 'blocks.4.attn.qkv.k_lorita', 'blocks.4.attn.qkv.v_lorita', 'blocks.5.attn.qkv', 'blocks.5.attn.qkv.q_lorita', 'blocks.5.attn.qkv.k_lorita', 'blocks.5.attn.qkv.v_lorita', 'blocks.6.attn.qkv', 'blocks.6.attn.qkv.q_lorita', 'blocks.6.attn.qkv.k_lorita', 'blocks.6.attn.qkv.v_lorita', 'blocks.7.attn.qkv', 'blocks.7.attn.qkv.q_lorita', 'blocks.7.attn.qkv.k_lorita', 'blocks.7.attn.qkv.v_lorita', 'blocks.8.attn.qkv', 'blocks.8.attn.qkv.q_lorita', 'blocks.8.attn.qkv.k_lorita', 'blocks.8.attn.qkv.v_lorita', 'blocks.9.attn.qkv', 'blocks.9.attn.qkv.q_lorita', 'blocks.9.attn.qkv.k_lorita', 'blocks.9.attn.qkv.v_lorita', 'blocks.10.attn.qkv', 'blocks.10.attn.qkv.q_lorita', 'blocks.10.attn.qkv.k_lorita', 'blocks.10.attn.qkv.v_lorita', 'blocks.11.attn.qkv', 'blocks.11.attn.qkv.q_lorita', 'blocks.11.attn.qkv.k_lorita', 'blocks.11.attn.qkv.v_lorita']\n",
      "Configured total parameters: 4316746\n",
      "Configured total memory (MB): 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4316746, 4.316746)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularizer =  main_helper.configure_model_experiment(model,cloned_config)\n",
    "main_helper.get_model_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb96756b-d362-4c72-8c98-bc488db458b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): TruncateLoRAQKVModule(\n",
       "          (q_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (k_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "          (v_lora): TruncateLoRA(in_features=192, out_features=192, rank=9)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c05c2d-6dc0-4826-8627-e5a3a6e7e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = argparse.Namespace(\n",
    "    dataset='CIFAR10',\n",
    "    model='VIT_Tiny',\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    epoch=50,\n",
    "    technique='Baseline',\n",
    "    rectangular_mode=True,\n",
    "    target_rank=4,\n",
    "    lmbda=0.01,\n",
    "    depth_lorita=4,\n",
    "    weight_decay_alpha=0.00,\n",
    "    dropout=0.0,\n",
    "    threads=4,\n",
    "    augmentation=False,\n",
    "    target_modules=['Q', 'K', 'V'],\n",
    "    DEVICE = torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6facb359-b7ef-44ad-9b75-888579edde61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "Base total # of parameters: 5526346\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n"
     ]
    }
   ],
   "source": [
    "size, labels, train_loader, validation_loader = main_helper.dataset_parser(\n",
    "        config)\n",
    "\n",
    "model = main_helper.model_parser(size, labels, config)\n",
    "optimizer, criterion = main_helper.optim_parser(model, config), torch.nn.CrossEntropyLoss()\n",
    "model = model.to(config.DEVICE)\n",
    "\n",
    "print(f\"Base total # of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "regulariser = main_helper.configure_model_experiment(model, config)  # converts it to LoRA, DoRA, or LoRiTA, Creates a\n",
    "# Regularizer Hoyer/Q3r/LoRiTa/LoRA/DoRA/etc targets the modules in a list ex [\"Q\",\"K,\"V\",\"mlp1\",\"mlp2\"]\n",
    "\n",
    "model = model.to(config.DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a4f6bb-cb75-486f-909e-a4be8368159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing truncated models as long\n",
      "Running truncated model 5.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 4247626\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 10.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 4316746\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 15.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 4385866\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 20.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 4454986\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 30.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 4579402\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 40.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 4717642\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 50.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 4855882\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 60.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 4980298\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 70.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 5118538\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 80.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 5242954\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n",
      "Running truncated model 90.0% original parameters on ['Q', 'K', 'V']\n",
      "Total parameters: 5526346\n",
      "Total memory (MB): 0.00\n",
      "NO TECHNIQUE APPLIED TO Baseline\n",
      "Configured total parameters: 5526346\n",
      "Configured total memory (MB): 0.00\n",
      "LoRITa Modules: []\n",
      "Configured total parameters: 5381194\n",
      "Configured total memory (MB): 0.00\n",
      "Truncated/LoRA model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss_0.05': tensor(183.9678, device='cuda:0'),\n",
       " 'test_acc_0.05': tensor(1132, device='cuda:0'),\n",
       " 'test_loss_0.1': tensor(184.4291, device='cuda:0'),\n",
       " 'test_acc_0.1': tensor(933, device='cuda:0'),\n",
       " 'test_loss_0.15': tensor(184.8240, device='cuda:0'),\n",
       " 'test_acc_0.15': tensor(1000, device='cuda:0'),\n",
       " 'test_loss_0.2': tensor(183.7673, device='cuda:0'),\n",
       " 'test_acc_0.2': tensor(1014, device='cuda:0'),\n",
       " 'test_loss_0.3': tensor(183.7250, device='cuda:0'),\n",
       " 'test_acc_0.3': tensor(1006, device='cuda:0'),\n",
       " 'test_loss_0.4': tensor(184.3895, device='cuda:0'),\n",
       " 'test_acc_0.4': tensor(1007, device='cuda:0'),\n",
       " 'test_loss_0.5': tensor(183.8412, device='cuda:0'),\n",
       " 'test_acc_0.5': tensor(1263, device='cuda:0'),\n",
       " 'test_loss_0.6': tensor(183.6963, device='cuda:0'),\n",
       " 'test_acc_0.6': tensor(1013, device='cuda:0'),\n",
       " 'test_loss_0.7': tensor(183.9238, device='cuda:0'),\n",
       " 'test_acc_0.7': tensor(1011, device='cuda:0'),\n",
       " 'test_loss_0.8': tensor(184.1488, device='cuda:0'),\n",
       " 'test_acc_0.8': tensor(1017, device='cuda:0'),\n",
       " 'test_loss_0.9': tensor(184.2407, device='cuda:0'),\n",
       " 'test_acc_0.9': tensor(1002, device='cuda:0'),\n",
       " 'test_loss_Baseline': tensor(183.8639, device='cuda:0'),\n",
       " 'test_acc_Baseline': tensor(1101, device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_helper.test_truncated_model(criterion, model, validation_loader, argparse.Namespace(size = (10000, 3, 224, 224),labels=10), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d24bc2-e725-42a8-b493-b09f8376e9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879b544-2ff7-4553-8c46-fb9cad73a056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
